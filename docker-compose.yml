version: '3.8'
services:
  pgmaster:
    container_name: pgmaster
    image: postgres
    restart: always
    networks:
      localnet:
        ipv4_address: 172.16.0.3
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5432:5432"
    volumes:
      - "./volumes/master/:/var/lib/postgresql/data"
      - "./pg/master/pg_hba.conf:/docker-entrypoint-initdb.d/config/master/pg_hba.conf"
      - "./pg/master/postgresql.conf:/docker-entrypoint-initdb.d/config/master/postgresql.conf"
      - "./pg/master/master-init.sh:/docker-entrypoint-initdb.d/master-init.sh"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 1s
      timeout: 1s
      retries: 30
  pgslave1:
    container_name: pgslave1
    image: postgres
    restart: always
    networks:
      localnet:
        ipv4_address: 172.16.0.4
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5433:5433"
    volumes:
      - "./volumes/slave1/:/var/lib/postgresql/data"
      - "./pg/slave1/standby.signal:/docker-entrypoint-initdb.d/config/slave1/standby.signal"
      - "./pg/slave1/postgresql.conf:/docker-entrypoint-initdb.d/config/slave1/postgresql.conf"
      - "./pg/slave1/slave1-init.sh:/docker-entrypoint-initdb.d/slave1-init.sh"
    links:
      - pgmaster
    depends_on:
      pgmaster:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 1s
      timeout: 1s
      retries: 30
  pgslave2:
    container_name: pgslave2
    image: postgres
    restart: always
    networks:
      localnet:
        ipv4_address: 172.16.0.5
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5434:5434"
    volumes:
      - "./volumes/slave2/:/var/lib/postgresql/data"
      - "./pg/slave2/standby.signal:/docker-entrypoint-initdb.d/config/slave2/standby.signal"
      - "./pg/slave2/postgresql.conf:/docker-entrypoint-initdb.d/config/slave2/postgresql.conf"
      - "./pg/slave2/slave2-init.sh:/docker-entrypoint-initdb.d/slave2-init.sh"
    links:
      - pgmaster
    depends_on:
      pgmaster:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 1s
      timeout: 1s
      retries: 30
  pgadmin:
    container_name: pgadmin
    image: dpage/pgadmin4
    networks:
      localnet:
        ipv4_address: 172.16.0.6
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: root@root.com
      PGADMIN_DEFAULT_PASSWORD: root
    ports:
      - "5050:80"
    volumes:
      - pgadmin:/var/lib/pas

#  pgpool:
#    image: bitnami/pgpool:latest
#    container_name: pgpool
#    environment:
#      - PGPOOL_BACKEND_NODES=0:pgmaster:5432,1:pgslave1:5433,2:pgslave2:5434
#      - PGPOOL_BACKEND_APPLICATION_NAMES=pgmaster,pgslave1,pgslave2
#      - PGPOOL_POSTGRES_USERNAME=postgres
#      - PGPOOL_POSTGRES_PASSWORD=postgres
#      - PGPOOL_HEALTH_CHECK_USER=postgres
#      - PGPOOL_HEALTH_CHECK_PASSWORD=postgres
#      - PGPOOL_SR_CHECK_USER=postgres
#      - PGPOOL_SR_CHECK_PASSWORD=postgres
#      - PGPOOL_ADMIN_USERNAME=admin
#      - PGPOOL_ADMIN_PASSWORD=admin
#      - PGPOOL_HEALTH_CHECK_PERIOD=2
#      - PGPOOL_HEALTH_CHECK_TIMEOUT=3
#      - PGPOOL_HEALTH_CHECK_MAX_RETRIES=2
#      - PGPOOL_SR_CHECK_PERIOD=2
#      # Failover/автовозврат
#      - PGPOOL_FAILOVER_ON_BACKEND_ERROR=yes
#      - PGPOOL_AUTO_FAILBACK=yes
#      - PGPOOL_AUTO_FAILBACK_INTERVAL=5
#      # Балансировка только для SELECT (по умолчанию да), записи — строго на мастер
#      - PGPOOL_ENABLE_LOAD_BALANCING=yes
#      # Таймауты клиентских сессий в pgpool (чтоб не висели вечно)
#      - PGPOOL_CONNECT_TIMEOUT=3
#      - PGPOOL_CLIENT_IDLE_LIMIT=60
#      - PGPOOL_CHILD_LIFE_TIME=30
#      - PGPOOL_CHILD_MAX_CONNECTIONS=100
#    ports:
#      - "5435:5432"
#    depends_on:
#      - pgmaster
#      - pgslave1
#      - pgslave2
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.24
  haproxy:
    image: haproxy:2.5
    container_name: haproxy
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
    ports:
      - "5001:5001"
    depends_on:
      - pgmaster
      - pgslave1
      - pgslave2
    networks:
      localnet:
        ipv4_address: 172.16.0.25

  nginx:
    image: nginx:1.21
    container_name: nginx-lb
    restart: on-failure
    volumes:
      - "./default.conf:/etc/nginx/conf.d/default.conf"
    ports:
      - "4088:80"
      - "4098:8080"
    networks:
      localnet:
        ipv4_address: 172.16.0.26
    links:
      - app
      - app2
      - app3
    depends_on:
      - app
      - app2
      - app3

  redis:
    image: redis:6.2
    container_name: redis
    networks:
      localnet:
        ipv4_address: 172.16.0.7
    ports:
      - "6379:6379"
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - "./volumes/redis/:/data"

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    networks:
      localnet:
        ipv4_address: 172.16.0.8
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    networks:
      localnet:
        ipv4_address: 172.16.0.9
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://kafka:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  app:
    build: .
    container_name: highloadApplication
    restart: always
    networks:
      localnet:
        ipv4_address: 172.16.0.2
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
      POSTGRES_HOST_MASTER: haproxy
      POSTGRES_PORT_MASTER: 5001
      REDIS_HOST: redis
      REDIS_PORT: 6379
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
    ports:
      - "8081:8081"
    links:
      - haproxy
      - kafka
      - pgmaster
      - pgslave1
      - pgslave2
    depends_on:
      - haproxy
      - kafka
      - pgmaster
      - pgslave1
      - pgslave2

  app2:
    build: .
    container_name: highloadApplication2
    restart: always
    networks:
      localnet:
        ipv4_address: 172.16.0.27
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
      POSTGRES_HOST_MASTER: haproxy
      POSTGRES_PORT_MASTER: 5001
      REDIS_HOST: redis
      REDIS_PORT: 6379
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
    ports:
      - "8084:8081"
    links:
      - haproxy
      - kafka
      - pgmaster
      - pgslave1
      - pgslave2
    depends_on:
      - haproxy
      - kafka
      - pgmaster
      - pgslave1
      - pgslave2

  app3:
    build: .
    container_name: highloadApplication3
    restart: always
    networks:
      localnet:
        ipv4_address: 172.16.0.28
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
      POSTGRES_HOST_MASTER: haproxy
      POSTGRES_PORT_MASTER: 5001
      REDIS_HOST: redis
      REDIS_PORT: 6379
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
    ports:
      - "8083:8081"
    links:
      - haproxy
      - kafka
      - pgmaster
      - pgslave1
      - pgslave2
    depends_on:
      - haproxy
      - kafka
      - pgmaster
      - pgslave1
      - pgslave2

  tarantool:
    container_name: tarantool
    image: tarantool/tarantool
    command: tarantool /opt/tarantool/app.lua
    environment:
      - TARANTOOL_USER_NAME=tarantool
      - TARANTOOL_USER_PASSWORD=tarantool
    ports:
      - "3301:3301"
    networks:
      localnet:
        ipv4_address: 172.16.0.22
    volumes:
      - ./hw7-tarantool/tarantool/init.lua:/opt/tarantool/app.lua
      - ./hw7-tarantool/tarantool/data:/var/lib/tarantool

  dialogtatanrool:
    build: ./hw7-tarantool/.
    container_name: dialogtatanrool
    restart: always
    networks:
      localnet:
        ipv4_address: 172.16.0.23
    environment:
      TARANTOOL_HOST: 172.16.0.22
      TARANTOOL_PORT: 3301
      USER_SERVICE_URL: http://172.16.0.2:8081
    ports:
      - "8082:8082"
    depends_on:
      - tarantool
#  # First Shard Cluster

#  mongoshard11:
#    build:
#      context: mongo/replicaset
#    container_name: mongoshard11
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.10
#    depends_on:
#      - mongoshard12
#      - mongoshard13
#    command: mongod --shardsvr --replSet shard1 --dbpath /data/db --port 27017
#    environment:
#      - REPLICA_SET=shard1
#    expose:
#      - "27017"
#    volumes:
#      - shard_d11:/data/db
#  mongoshard12:
#    image: mongo
#    container_name: mongoshard12
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.11
#    command: mongod --shardsvr --replSet shard1 --dbpath /data/db --port 27017
#    expose:
#      - "27017"
#    volumes:
#      - shard_d12:/data/db
#  mongoshard13:
#    image: mongo
#    container_name: mongoshard13
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.12
#    command: mongod --shardsvr --replSet shard1 --dbpath /data/db --port 27017
#    expose:
#      - "27017"
#    volumes:
#      - shard_d13:/data/db
#
#  # Second Shard Cluster
#
#  mongoshard21:
#    build:
#      context: mongo/replicaset
#    container_name: mongoshard21
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.13
#    depends_on:
#      - mongoshard22
#      - mongoshard23
#    command: mongod --shardsvr --replSet shard2 --dbpath /data/db --port 27017
#    environment:
#      - REPLICA_SET=shard2
#    expose:
#      - "27017"
#    volumes:
#      - shard_d21:/data/db
#  mongoshard22:
#    image: mongo
#    container_name: mongoshard22
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.14
#    command: mongod --shardsvr --replSet shard2 --dbpath /data/db --port 27017
#    expose:
#      - "27017"
#    volumes:
#      - shard_d22:/data/db
#  mongoshard23:
#    image: mongo
#    container_name: mongoshard23
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.15
#    command: mongod --shardsvr --replSet shard2 --dbpath /data/db --port 27017
#    expose:
#      - "27017"
#    volumes:
#      - shard_d23:/data/db
#
#    # Config Cluster
#
#  mongocfg1:
#    build:
#      context: mongo/replicaset
#    container_name: mongocfg1
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.16
#    depends_on:
#      - mongocfg2
#      - mongocfg3
#    command: mongod --configsvr --replSet cfg --dbpath /data/db --port 27017
#    environment:
#      - REPLICA_SET=cfg
#    expose:
#      - "27017"
#    volumes:
#      - shard_c1:/data/db
#  mongocfg2:
#    image: mongo
#    container_name: mongocfg2
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.17
#    command: mongod --configsvr --replSet cfg --dbpath /data/db --port 27017
#    expose:
#      - "27017"
#    volumes:
#      - shard_c2:/data/db
#  mongocfg3:
#    image: mongo
#    container_name: mongocfg3
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.18
#    command: mongod --configsvr --replSet cfg --dbpath /data/db --port 27017
#    expose:
#      - "27017"
#    volumes:
#      - shard_c3:/data/db
#
#  # Build mongos Router
#
#  mongos1:
#    build:
#      context: mongo/router
#    container_name: mongos1
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.19
#    depends_on:
#      - mongocfg1
#      - mongocfg2
#      - mongocfg3
#      - mongoshard11
#      - mongoshard12
#      - mongoshard13
#      - mongoshard21
#      - mongoshard22
#      - mongoshard23
#    command: mongos --configdb cfg/mongocfg1:27017,mongocfg2:27017,mongocfg3:27017 --port 27017 --bind_ip_all
#    environment: #we can specify only the primary node for each cluster shard, other nodes will appear automatically
#      - SHARDS=shard1/mongoshard11;shard2/mongoshard21
#    ports:
#      - 27027:27017
#    expose:
#      - "27017"
#  mongo-express:
#    image: mongo-express
#    container_name: mongo-express
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.20
#    depends_on:
#      - mongos1
#    environment:
#      ME_CONFIG_MONGODB_SERVER: mongos1
#      ME_CONFIG_MONGODB_PORT: 27017
#      ME_CONFIG_MONGODB_ADMINUSERNAME: ""
#      ME_CONFIG_MONGODB_ADMINPASSWORD: ""
#    ports:
#      - "8090:8081"

#  dialog:
#    build: ./hw5-sharding/.
#    container_name: dialogHighload
#    restart: always
#    networks:
#      localnet:
#        ipv4_address: 172.16.0.21
#    environment:
#      MONGO_HOST: mongos1
#      MONGO_PORT: 27017
#      MONGO_DB_NAME: chatApp
#    ports:
#      - "8082:8082"
#    depends_on:
#      - mongos1
#      - app

volumes:
  pgmaster:
  pgslave1:
  pgslave2:
  pgadmin:
  redis:
  kafka:
  haproxy:
#  pgpool:
  app:
#  shard_d11:
#  shard_d12:
#  shard_d13:
#  shard_d21:
#  shard_d22:
#  shard_d23:
#  shard_c1:
#  shard_c2:
#  shard_c3:
#  dialog:

networks:
  localnet:
    driver: bridge
    ipam:
      config:
        - subnet: 172.16.0.0/16
          gateway: 172.16.0.1